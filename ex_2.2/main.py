#!/usr/bin/env python3
# Copyright 2019 Christian Henning, Alexander Meulemans
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Controller for simulations
--------------------------

The module :mod:`main` is an executable script that controls the simulations
(i.e., the training of regression tasks).

For more usage information, please check out

.. code-block:: console

  $ python3 main --help

There are two distinct regression tasks that we consider here.

Student-teacher model
^^^^^^^^^^^^^^^^^^^^^

The data for this regression task is generated by a teacher model. Once random
input patterns are generated, a teacher network is used to map these inputs
onto target outputs. In this way, a dataset consisting of inputs and outputs is
generated, that represents a nonlinear regression task (assuming the teacher
network is nonlinear). For more specifics on the dataset generation, have a look
at the function :func:`lib.utils.generate_data_from_teacher`.

Once the dataset is generated, a student network can be trained to mimic the
behavior of the teacher network. In case the teacher and student share an
identical architecture, we know there is a perfect solution that can be found
by the employed optimization algorithm (we just need to find the weights of
the teacher model when training the student model).

We will be looking at different architectures for student and teacher model such
that it is likely that we consider a challenging regression task.

1D polynomial regression
^^^^^^^^^^^^^^^^^^^^^^^^

In case of 1D regression tasks, the result can be visualized to a human, who
can assess the quality of the optimizer.

An example 1D regression task can be retrieved using the function
:func:`lib.utils.regression_cubic_poly`.

This script can be used to train on this dataset via the option
``--polynomial_regression``.

.. code-block:: console

  $ python main.py --polynomial_regression

.. autosummary::

    main.run
    main.train
"""
import argparse
import numpy as np
import random
import torch
from torch.utils.data import DataLoader

import lib.backprop_functions as bf
from lib.mlp import MLP
from lib import utils

def test(device, test_loader, net):
    """test a train network by computing the MSE on the test set.

    Args:
        (....): See docstring of function :func:`train`.
        test_loader (torch.utils.data.DataLoader): The data handler for
            test data.

    Returns:
        (float): The mean-squared error for the test set ``test_loader`` when
        using the network ``net``. Note, the ``Function``
        :func:`lib.backprop_functions.mse_loss` is used to compute the MSE
        value.
    """
    #######################################################################
    ### NOTE, the function `mse_loss` divides by the current batch size. In
    ### order to compute the MSE across several mini-batches, one needs to
    ### correct for this behavior.
    #######################################################################

    net.eval()

    with torch.no_grad():
        num_samples = 0
        mse = 0.

        for i, (inputs, targets) in enumerate(test_loader):
            inputs, targets = inputs.to(device), targets.to(device)

            batch_size = int(inputs.shape[0])
            num_samples += batch_size

            predictions = net.forward(inputs)

            mse += batch_size * bf.mse_loss(predictions, targets)

        mse /= num_samples

    print('Test MSE: {}'.format(mse))

    return float(mse.cpu().detach().numpy())

def train(args, device, train_loader, net):
    """Train the given network on the given (regression) dataset.

    Args:
        args (argparse.Namespace): The command-line arguments.
        device: The PyTorch device to be used.
        train_loader (torch.utils.data.DataLoader): The data handler for
            training data.
        net: The (student) neural network.
    """
    print('Training network ...')
    net.train()

    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr,
                                momentum=args.momentum)
    if args.plot_matrix_angles:
        angles_transpose = torch.empty(args.epochs, net.depth,
                                       requires_grad=False)
        angles_pinv = torch.empty(args.epochs, net.depth,
                                  requires_grad=False)
        angles_contr = torch.empty(args.epochs, net.depth,
                                  requires_grad=False)


    for e in range(args.epochs):
        for i, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(device), targets.to(device)

            predictions = net.forward(inputs)

            optimizer.zero_grad()

            loss = bf.mse_loss(predictions, targets)
            loss.backward()
            optimizer.step()

        # Save the feedback alignment angles
        if args.plot_matrix_angles:
            for i in range(net.depth):
                W = net.linear_layers[i+1].weights
                B = net.linear_layers[i+1].feedbackweights
                W_pinv = torch.pinverse(W)
                angles_transpose[e,i] = utils.compute_matrix_angle(W.t(), B)
                angles_pinv[e,i] = utils.compute_matrix_angle(W_pinv, B)
                angles_contr[e, i] = utils.compute_matrix_angle(W_pinv, W.t())

        if (e+1) % 10 == 0:
            print('Epoch {} -- loss = {}.'.format(e+1, loss))

    print('Training network ... Done')

    if args.plot_matrix_angles:
        print('Plotting matrix angles ...')
        utils.plot_angles(angles_transpose,
                          r'angle between $W_i^T$ and $B$',
                          r'$B_{%s} \angle W_{%s}^{T} [\circ]$')
        utils.plot_angles(angles_pinv,
                          r'angles between $W_i^{\dagger}$ and $B$',
                          r'$B_{%s} \angle W_{%s}^{\dagger} [\circ]$')


def run():
    """Run the script.

    - Parsing command-line arguments
    - Creating synthetic regression data
    - Initiating training process
    - Testing final network
    """
    ### Parse CLI arguments.
    parser = argparse.ArgumentParser(description='Nonlinear regression with ' +
                                     'neural networks.')

    rgroup = parser.add_argument_group('Regression options')
    rgroup.add_argument('--polynomial_regression', action='store_true',
                        help='Perform a 1D polynomial regression instead of ' +
                             'using a dataset that has been obtained from a ' +
                             'teacher model.')

    tgroup = parser.add_argument_group('Training options')
    tgroup.add_argument('--epochs', type=int, metavar='N', default=100,
                        help='Number of training epochs. ' +
                             'Default: %(default)s.')
    tgroup.add_argument('--batch_size', type=int, metavar='N', default=32,
                        help='Training batch size. Default: %(default)s.')
    tgroup.add_argument('--lr', type=float, default=1e-4,
                        help='Learning rate of optimizer. Default: ' +
                             '%(default)s.')
    tgroup.add_argument('--momentum', type=float, default=0.0,
                        help='Momentum of the optimizer. ' +
                             'Default: %(default)s.')
    tgroup.add_argument('--feedback_alignment', action='store_true',
                        help='Use feedback alignment to train the network' +
                             'instead of backpropagation')

    sgroup = parser.add_argument_group('Network options')
    sgroup.add_argument('--num_hidden', type=int, metavar='N', default=2,
                        help='Number of hidden layer in the (student) ' +
                             'network. Default: %(default)s.')
    sgroup.add_argument('--size_hidden', type=int, metavar='N', default=10,
                        help='Number of units in each hidden layer of the ' +
                             '(student) network. Default: %(default)s.')
    sgroup.add_argument('--size_input', type=int, metavar='N', default=5,
                        help='Number of units of the input'
                             '. Default: %(default)s.')
    sgroup.add_argument('--size_output', type=int, metavar='N', default=5,
                        help='Number of units of the output'
                             '. Default: %(default)s.')
    sgroup.add_argument('--linear', action='store_true',
                        help='Train a linear network on a linear dataset')

    pgroup = parser.add_argument_group('Plotting options')
    pgroup.add_argument('--plot_matrix_angles', action='store_true',
                        help='Show a plot of the angles between B, W^T and '
                             'pseudoinverse(W)')
    pgroup.add_argument('--show_plot', action='store_true',
                        help='Show the final regression results as plot. ' +
                             'Note, only applies to 1D regression tasks.')


    mgroup = parser.add_argument_group('Miscellaneous options')
    mgroup.add_argument('--use_cuda', action='store_true',
                        help='Flag to enable GPU usage.')
    mgroup.add_argument('--random_seed', type=int, metavar='N', default=42,
                        help='Random seed. Default: %(default)s.')

    args = parser.parse_args()
    if not args.feedback_alignment:
        args.plot_matrix_angles = False

    ### Ensure deterministic computation.
    torch.manual_seed(args.random_seed)
    torch.cuda.manual_seed_all(args.random_seed)
    np.random.seed(args.random_seed)
    random.seed(args.random_seed)

    # Ensure that runs are reproducible even on GPU. Note, this slows down
    # training!
    # https://pytorch.org/docs/stable/notes/randomness.html
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    use_cuda = args.use_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    print('Using cuda: ' + str(use_cuda))

    ### Generate datasets and data handlers.
    if args.polynomial_regression:
        print('### Learning to regress a 1D cubic polynomial ###')
        n_in = n_out = 1

        train_x, test_x, train_y, test_y = utils.regression_cubic_poly()

    else:
        print('### Training a student model to mimic a teacher ###')
        n_in = args.size_input
        n_out = args.size_output

        train_x, test_x, train_y, test_y = utils.generate_data_from_teacher( \
            n_in=n_in, n_out=n_out, n_hidden=[25,50,25], linear=False,
            num_train=1000)

    train_loader = DataLoader(utils.RegressionDataset(train_x, train_y),
                              batch_size=args.batch_size, shuffle=True)
    test_loader = DataLoader(utils.RegressionDataset(test_x, test_y),
                             batch_size=args.batch_size, shuffle=False)

    ### Generate network.
    n_hidden = [args.size_hidden] * args.num_hidden
    net = MLP(n_in=n_in, n_out=n_out, n_hidden=n_hidden,
              fa=args.feedback_alignment,
              linear=args.linear).to(device)

    ### Train network.
    train(args, device, train_loader, net)

    ### Test network.
    test(device, test_loader, net)

    if args.show_plot and n_in == 1 and n_out == 1:
        utils.plot_predictions(device, test_loader, net)

if __name__ == '__main__':
    run()


